# 2020-6-30		周二		晴

<style>
p {  
text-indent:2em;
}
</style>

## glidedskypython爬虫闯关

唉，又月底了！

之前有在做一个[http://glidedsky.com/]上面的python爬虫闯关，但是一直卡在第三题，

```

大家说，页面做分页是因为单页内容太多。但分页还有一个不为人知的好处——用来反爬。
当一个爬虫尝试不断翻页爬取所有内容的时候，行为特征会非常容易识别。比如说，对高频率访问的IP进行封禁。
因为这是一个硬核的爬取攻防练习，常规高频封禁太弱了，所以这里的策略是：你的每个IP，只能访问一次，之后就会被封禁。
悄悄地告诉你，你之前用过的IP，已经被悄悄记录了~
这里有一个网站，分了1000页，求所有数字的和。
待爬取网站
```

尝试去做过一个小型的代理池，无奈只知道两个免费代理的网址，西刺代理和快代理，但很显然，能用的不多，尝试了下100个中只有几个能用，而且是前面的比较新的，后面的年代久远估计效果也不会太好。就给放弃了。想过用过别人的答案但是都失败了，看来还是很有意思很有难度的。每个人的数据可能但是随机生成的。

今天发现了个代理池项目，去测试了下可用的还是比较可观的，而且有多个网站在实时抓取，效率也还行。

就又去尝试了。

第一次尝试的时候只留意了一第一页的url然后就自信的去构造后续的url进行爬取了，但是爬取的时候发现返回的都是404,不对啊，去看网页的情况，哪怕使用了重复的代理也应该是403啊。自己在本地切换了个代理去看第二页的，果然和自己的不一样。

在解决完上面的问题后就去进行爬取了，也没有说先测试两个比对下看看对不对，从早上到下午爬取了400多页还挺高兴，结果去随机看两个html的代码，一模一样，u因为代码结构的问题导致url后面的页码的自动递加变更没有生效，也就是一直在爬取第一页，而且ip是不断被记录的，也就是白白浪费了400多个可用代理。

由于晚上会断电，自己本地完成是不可能的了，还好有服务器，决定部署到服务器上面去，管他运行几天几夜也没事。

最开始用最普通的慢慢下载库，配置环境，最后折腾来折腾去还是因为环境问题失败了。后面去配置docker，用docker就很安逸，一路畅通无阻。不得不感慨，docker还是好用啊

# 2020-7-8  周三   晴

## redis无法关闭

通过htop查询进程尝试关闭没用，再次查询又重启新开了一个
记下进程序号通过kill命令关闭也无法关闭
进入redis后按照正常流程去关闭退出也没用

最终下面的命令解决
sudo /etc/init.d/redis-server stop

# 2020-7-10   周五  雨

## fake_useragent库的问题

之前一直认为这个库就像是像开始那会自己定制的小型useragent池的放大，也就是所有的useragent都存在本地，在用的时候再弹出来而已，今天遇到了一些问题，

```
fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached
```

大概意思是：这个模块尝试请求一个东西已达到最大重试次数

打开这个模块的源码进行查看发现这个库会引用在线资源，所以这个模块是进行几次尝试请求一个网站的 Json 数据，但是因为各种原因请求超时，所以就会报这个错误

```
解决方案：
首先第一步要进行更新 fake_useragent
pip install --upgrade fake_useragent
1、在实例化的时候指定一些参数

禁用服务器缓存
ua = UserAgent(use_cache_server=False)

不缓存数据
ua = UserAgent(cache=False)

忽略ssl验证
ua = UserAgent(verify_ssl=False)
```

>一般的话，通过上述解决方案都能解决了，如果还没解决…
使用临时 Json 文件
在 fake_useragent\settings.py 发现了几个 URL，其中有一些是打不开的，所以，我们将能打开的 URL 的 Json 文件保存在本地
wget https://fake-useragent.herokuapp.com/browsers/0.1.11
这时我们就会得到一个 0.1.11 的文件，将文件名改为 fake_useragent_0.1.11.json
mv 0.1.11 fake_useragent_0.1.11.json
再通过自己构造方法就可以提取里面的useragent先暂时使用着

# 2020-7-11  周六  阴  

## 过滤文件名中不能包含的字符

又是遇到了以前遇到的一个问题，在直接截取文章文件名给文件命名的时候报错中断，没有考虑到包含有不符合命名要求的字符。以前一直都是构建for循环加if判断语句然后replace()方法去替换，那时候没有考虑效率的问题，现在想想自己去构造这个方法挺麻烦的，而且不断的判断，循环在长时间，重复性的肯定也浪费了不少时间。

今天去仔细看了下，利用正则的sub方法可以很好的解决这个问题：

```
import re
new_name = re.sub(r'[\/:*?"<>|]','_', old_name)
```

# 2020-7-19  周日  晴

## beautifulsoup的string、strings、text、get_text、stripped_strings区别

string：用来获取目标路径下第一个非标签字符串，得到的是个字符串
最开始看到有string，但是自己去尝试时匹配得到的为空,大概为这样的，想要获取a标签内的内容返回空

```
<a><div></div>匹配信息</a>
```

后经过测试，这个是直接匹配a标签后的字符串，而这个字符串被另一个标签隔开了所以为空

strings：用来获取目标路径下所有的子孙非标签字符串，返回的是个生成器

stripped_strings：用来获取目标路径下所有的子孙非标签字符串，会自动去掉空白字符串，返回的是一个生成器
在用strings获取到然后利用索引后得到的字符串是不符合要求的，好有很多空格，早期利用一些方法去去掉多余的空格：

```
1、使用字符串函数replace
str = 'hello world'
str.replace(' ', '')
'helloworld'

2、使用字符串函数split
a = str.split()

3、使用正则表达式
import re
strinfo = re.compile()
strinfo = re.compile(' ')
b = strinfo.sub('', a)

有了stripped_strings后便可以不用这么麻烦了
```

get_text：用来获取目标路径下的子孙字符串，返回的是字符串（包含HTML的格式内容）

text（不建议使用）：用来获取目标路径下的子孙非标签字符串，返回的是字符串
	beautifulsoup中，对外接口，没有提供text这个属性，只有string这个属性值；
	beautifulsoup内部才有text这个属性，只供内部使用 –> 如果你想要用text值，应该调用对应的get_text()
	而你之所有能够直接用soup.text而没报错，应该是和python的class的property没有变成private有关系 –>导致你外部也可以访问到这个，本身是只供内部使用的属性值-> 这个要抽空深究了。

## ubuntu系统广告

也不算问题吧，但是今天突然对这个感兴趣了去查了一下
起因是因为自己以前最开始用的是ubnutu。然后选择服务器的时候自然而然的选择了ubuntu的，在每次连接上服务器的时候，都会弹出一些提示什么的
![image-20200719053400170](fw-ad.png)

以前是进去了就直接做自己的事了，今天刚好有空别且觉得这个提示怎么那么奇怪呢，ubuntu的系统整macos的东西
去查了一下，果不起然是一个广告，据说最早的是在2017年就有人发现并提出了，但是至今2020年ubuntu公司没有就此事作出回应，也没有解决这个问题

再去查查推广的那个链接里面的是什么，得到的如下：
<u>**Microk8s 是本地部署 Kubernetes 集群的 'click-and-run' 方案，最初由 Ubuntu 的发布者 Canonical 开发。**</u>
这也就能理解了，借用自家的平台做自己的推广，但是这种方式还是对很大一部分用户造成了不好的影响，有违linux的理念

去除方法：

```
只需禁用 motd-news.service 和 motd-news.timer 这两个系统组件，然后根据所使用的 Ubuntu 版本删除以下文件即可：

/etc/update-motd.d/99-esm＃Ubuntu 14.04
/etc/update-motd.d/10-help-text＃Ubuntu 14.04+
/etc/update-motd.d/50-motd-news＃Ubuntu 16.04+
/etc/update-motd.d/80-esm＃Ubuntu 16.04+
/etc/update-motd.d/80-livepatch＃Ubuntu 18.04+

# 这个版本号和内容名称不一定是一一对应的，我用的18.04,但是内容是50-motd-news
```


# 2020-7-30		周四		晴

## docker概念纠错

最开始了解到docker之后去看了一些资料，当时误把容器当作应用或者一个应用一个容器了，一在在思考这样的话那不同的应用之间不就是不能交互了吗

今天去仔细看了，不是那个意思，一个容器指的是一个运行环境，比如说python的运行环境，golang的运行环境，一个网页的运行环境，就算他们都需要数据库，但是因为是不同的容器的，相互是独立的，也就不会互相影响

# 2020-8-4        周二        晴
## markdown的较为高级的使用
由于之前使用的一直都是typora，导致其实最开始去学习的markdown语法并不是很熟悉，平常写的时候都是像word那样思考并去写的，写一些东西，然后一些快捷键去格式化成为自己想要的标题、段落、代码段等等，也就是说最开始使用的不过是一个特殊的记事本而已，这几天给换成了vnote，编辑和预览完全分开了，从代码编辑，才算是真正见识到了markdown的真容
基本现在的markdown编辑器都会提供快捷键去提供更好的用户体验以及更高效的编辑，但是对于真正使用markdown来说，可以像普通编辑器编辑文本那样编辑，但是如果那样去思考的话我觉得属实有些不好
markdown是兼容html的，所以应该像html那样去思考
段落的缩进，字体的颜色等等都是可以改变的
链接也不仅仅限于网页链接，本地文件也是可以的


# 2020-8-5       周三        晴
## 解除网页的复制、不允许转载限制
想要复制知乎的两篇文章到自己的笔记中，但是作者设置了禁止转载，但文章资料确实很好，自己也不是用做商业用途，基本是给自己看的，就看看有没有什么方法可以解除这个限制，最后查到一个方法，只要禁用js就可以了
![Screen Capture_select-area_20200805133737](fw_copy.gif)


# 2020-8-27  周四  晴
## 避免’sudo echo x >’ 时’Permission denied’

甲： 示例
sudo echo a > 1.txt
-bash: 1.txt: Permission denied
乙： 分析：
bash 拒绝这么做，说是权限不够.
这是因为重定向符号 “>” 也是 bash 的命令。sudo 只是让 echo 命令具有了 root 权限，
但是没有让 “>” 命令也具有root 权限，所以 bash 会认为这个命令没有写入信息的权限。
丙： 解决办法。三种：

利用 “sh -c” 命令，它可以让 bash 将一个字串作为完整的命令来执行，这样就可以将 sudo 的影响范围扩展到整条命令。
具体用法如下：
sudo sh -c “echo a > 1.txt”
利用bash -c 也是一样的，现在bash shell 流行。

利用管道和 tee 命令，该命令可以从标准输入中读入信息并将其写入标准输出或文件中，
具体用法如下：
echo a |sudo tee 1.txt
echo a |sudo tee -a 1.txt // -a 是追加的意思，等同于 >>
tee 命令很好用，它从管道接受信息，一边向屏幕输出，一边向文件写入。
linux 总是有一些小工具为我们考虑的很贴切！

提升shell 权限。
sudo -s //提到root 权限。提示符为#
当你觉得该退回到普通权限时，
sudo su username //退回到username 权限，提示符为$
exit 退出当前用户，回到上一层目录.

# 2020-10-14   周三   雨
## 给markdown文件生成目录
在文章开始地方输入[toc]，即可在对应位置插入目录


google-chrome 在不能通过设置全局代理的 linux 设备上直接使用代理
```
google-chrome-stable --proxy-server=[<proxy-scheme>://]<proxy-host>[:<proxy-port>
例 ：  --proxy-server=socks://127.0.0.1:1080
```
之后在上插件商店安装 SwitchyOmega 配置即可，下一次就会通过SwitchyOmega设置代理了，无需命令行启动了
多用户的话，要设置代理的用户最后关闭，命令行启动会默认打开最后一次关闭的用户